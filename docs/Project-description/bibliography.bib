@online{ghostcellpattern,
    title="Ghost Cell Pattern",
    author="Fredrik Berg Kjolstad, Marc Snir",
}

@book{openaccbestpractices,
    title="OpenACC Programming and Best Practises Guide",
    url="openacc-standard.org",
    author="OpenACC",
}

@article{nfs,
    title="Network File System",
    url="https://en.wikipedia.org/wiki/Network_File_System",
    author="Wikipedia",
}

@article{mergesort,
    title="Merge Sort",
    url="https://en.wikipedia.org/wiki/Merge_sort",
    author="Wikipedia",
}

@article{mpi-ethernet,
title = {Analyzing MPI performance over 10-Gigabit ethernet},
journal = {Journal of Parallel and Distributed Computing},
volume = {65},
number = {10},
pages = {1253-1260},
year = {2005},
note = {Design and Performance of Networks for Super-, Cluster-, and Grid-Computing Part I},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2005.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S074373150500105X},
author = {Justin (Gus) Hurwitz and Wu-chun Feng},
keywords = {10GbE, Ethernet, MPI, SAN, Interconnect, Cluster},
abstract = {Recent work with 10-Gigabit (10GbE) network adapters has demonstrated good performance in TCP/IP-based local- and wide-area networks (LANs and WANs). In the present work we present an evaluation of host-based 10GbE adapters in a system-area network (SAN) in support of a cluster. This evaluation focuses on the performance of the message-passing interface (MPI) when running over a 10GbE interconnect. We find that MPI over 10GbE provides communications performance comparable to that of TCP alone and fairly competitive with more exotic technologies such as MPI over Quadrics. The optimization of MPI and MPI-based applications to make use of this performance, however, is a non-trivial task. Consequently, it is difficult for MPI-based applications to realize this performance when running current-generation 10GbE hardware.}
}

